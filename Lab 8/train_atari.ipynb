{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVwo9IqHuWdW"
      },
      "outputs": [],
      "source": [
        "# STUDENT NUMBERS\n",
        "# 1886648\n",
        "# 1851234 \n",
        "# 1669326\n",
        "from IPython.display import JSON\n",
        "from google.colab import output\n",
        "from subprocess import getoutput\n",
        "import os\n",
        "\n",
        "def shell(command):\n",
        "  if command.startswith('cd'):\n",
        "    path = command.strip().split(maxsplit=1)[1]\n",
        "    os.chdir(path)\n",
        "    return JSON([''])\n",
        "  return JSON([getoutput(command)])\n",
        "output.register_callback('shell', shell)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Isb50y8Pu427"
      },
      "outputs": [],
      "source": [
        "#@title Colab Shell\n",
        "%%html\n",
        "<div id=term_demo></div>\n",
        "<script src=\"https://code.jquery.com/jquery-latest.js\"></script>\n",
        "<script src=\"https://cdn.jsdelivr.net/npm/jquery.terminal/js/jquery.terminal.min.js\"></script>\n",
        "<link href=\"https://cdn.jsdelivr.net/npm/jquery.terminal/css/jquery.terminal.min.css\" rel=\"stylesheet\"/>\n",
        "<script>\n",
        "  $('#term_demo').terminal(async function(command) {\n",
        "      if (command !== '') {\n",
        "          try {\n",
        "              let res = await google.colab.kernel.invokeFunction('shell', [command])\n",
        "              let out = res.data['application/json'][0]\n",
        "              this.echo(new String(out))\n",
        "          } catch(e) {\n",
        "              this.error(new String(e));\n",
        "          }\n",
        "      } else {\n",
        "          this.echo('');\n",
        "      }\n",
        "  }, {\n",
        "      greetings: 'Welcome to Colab Shell',\n",
        "      name: 'colab_demo',\n",
        "      height: 250,\n",
        "      prompt: 'colab > '\n",
        "  });"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DYNYL1l0sEMd"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import gym\n",
        "import torch\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0,\"/content/drive/MyDrive/Colab Notebooks\")\n",
        "\n",
        "from dqn.agent import DQNAgent\n",
        "from dqn.replay_buffer import ReplayBuffer\n",
        "from dqn.wrappers import *\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    hyper_params = {\n",
        "        \"seed\": 42,  # which seed to use\n",
        "        \"env\": \"PongNoFrameskip-v4\",  # name of the game\n",
        "        \"replay-buffer-size\": int(5e3),  # replay buffer size\n",
        "        \"learning-rate\": 1e-4,  # learning rate for Adam optimizer\n",
        "        \"discount-factor\": 0.99,  # discount factor\n",
        "        \"num-steps\": int(1e6),  # total number of steps to run the environment for\n",
        "        \"batch-size\": 256,  # number of transitions to optimize at the same time\n",
        "        \"learning-starts\": 10000,  # 10000 number of steps before learning starts\n",
        "        \"learning-freq\": 5,  # number of iterations between every optimization step\n",
        "        \"use-double-dqn\": False,  # use double deep Q-learning\n",
        "        \"target-update-freq\": 1000,  # 1000 number of iterations between every target network update\n",
        "        \"eps-start\": 1.0,  # e-greedy start threshold\n",
        "        \"eps-end\": 0.01,  # e-greedy end threshold\n",
        "        \"eps-fraction\": 0.1,  # fraction of num-steps\n",
        "        \"print-freq\": 10,\n",
        "    }\n",
        "\n",
        "    np.random.seed(hyper_params[\"seed\"])\n",
        "    random.seed(hyper_params[\"seed\"])\n",
        "\n",
        "    assert \"NoFrameskip\" in hyper_params[\"env\"], \"Require environment with no frameskip\"\n",
        "    env = gym.make(hyper_params[\"env\"])\n",
        "    env.seed(hyper_params[\"seed\"])\n",
        "\n",
        "    env = NoopResetEnv(env, noop_max=30)\n",
        "    env = MaxAndSkipEnv(env, skip=4)\n",
        "    env = EpisodicLifeEnv(env)\n",
        "    env = FireResetEnv(env)\n",
        "    # TODO Pick Gym wrappers to use\n",
        "    #\n",
        "    #\n",
        "    #\n",
        "    env = WarpFrame(env)\n",
        "    env = PyTorchFrame(env)\n",
        "    env = gym.wrappers.Monitor(env, \"Storage/recordings\", force = True)\n",
        "\n",
        "    replay_buffer = ReplayBuffer(hyper_params[\"replay-buffer-size\"])\n",
        "\n",
        "    # TODO Create dqn agent\n",
        "    # agent = DQNAgent( ... )\n",
        "    agent = DQNAgent(env.observation_space, env.action_space, replay_buffer, hyper_params[\"use-double-dqn\"], hyper_params[\"learning-rate\"], hyper_params[\"batch-size\"], hyper_params[\"discount-factor\"])\n",
        "\n",
        "    eps_timesteps = hyper_params[\"eps-fraction\"] * float(hyper_params[\"num-steps\"])\n",
        "    episode_rewards = [0.0]\n",
        "    losses = []\n",
        "    episode_losses = [0.0]\n",
        "\n",
        "    state = env.reset()\n",
        "    for t in range(hyper_params[\"num-steps\"]):\n",
        "        fraction = min(1.0, float(t) / eps_timesteps)\n",
        "        eps_threshold = hyper_params[\"eps-start\"] + fraction * (\n",
        "            hyper_params[\"eps-end\"] - hyper_params[\"eps-start\"]\n",
        "        )\n",
        "        sample = random.random()\n",
        "        # TODO\n",
        "        #  select random action if sample is less equal than eps_threshold\n",
        "        # take step in env\n",
        "        # add state, action, reward, next_state, float(done) to reply memory - cast done to float\n",
        "        # add reward to episode_reward\n",
        "\n",
        "        action = agent.act(state, sample, eps_threshold)\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        done = float(done)\n",
        "        agent.memory.add(state, action, reward, next_state, done)\n",
        "        state = next_state\n",
        "\n",
        "        episode_rewards[-1] += reward\n",
        "        if done:\n",
        "            state = env.reset()\n",
        "            episode_rewards.append(0.0)\n",
        "            episode_losses.append(0.0)\n",
        "\n",
        "        if (\n",
        "            t > hyper_params[\"learning-starts\"]\n",
        "            and t % hyper_params[\"learning-freq\"] == 0\n",
        "        ):\n",
        "            losses.append(agent.optimise_td_loss())\n",
        "            episode_losses[-1] += losses[-1]\n",
        "\n",
        "        if (\n",
        "            t > hyper_params[\"learning-starts\"]\n",
        "            and t % hyper_params[\"target-update-freq\"] == 0\n",
        "        ):\n",
        "            agent.update_target_network()\n",
        "\n",
        "        num_episodes = len(episode_rewards)\n",
        "\n",
        "        if (\n",
        "            done\n",
        "            and hyper_params[\"print-freq\"] is not None\n",
        "            and len(episode_rewards) % hyper_params[\"print-freq\"] == 0\n",
        "        ):\n",
        "            mean_100ep_reward = round(np.mean(episode_rewards[-101:-1]), 1)\n",
        "            print(\"********************************************************\")\n",
        "            print(\"steps: {}\".format(t))\n",
        "            print(\"episodes: {}\".format(num_episodes))\n",
        "            print(\"mean 100 episode reward: {}\".format(mean_100ep_reward))\n",
        "            print(\"% time spent exploring: {}\".format(int(100 * eps_threshold)))\n",
        "            print(\"********************************************************\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20HUG2Qbo-pD"
      },
      "outputs": [],
      "source": [
        "torch.save(agent.policy_net, \"Storage/policy_net.pb\")\n",
        "torch.save(agent.target_net, \"Storage/target_net.pb\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcK9vth8yEyp"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "figure, plots = plt.subplots(3, 1, figsize=(20, 15))\n",
        "\n",
        "episodeLengths = env.get_episode_lengths()\n",
        "npEpisode_losses = np.array(episode_losses)\n",
        "npEpisodeLengths = np.array(episodeLengths, dtype = np.float32)\n",
        "averaged_episode_losses = []\n",
        "\n",
        "if len(npEpisode_losses) != len(npEpisodeLengths):\n",
        "  averaged_episode_losses = npEpisode_losses[:-2] / npEpisodeLengths\n",
        "else:\n",
        "  averaged_episode_losses = npEpisode_losses / npEpisodeLengths\n",
        "\n",
        "figure.suptitle(\"Graphs\")\n",
        "\n",
        "plots[0].set_ylabel(\"Reward per Episode\")\n",
        "plots[0].set_xlabel(\"Episode\")\n",
        "plots[0].plot(np.arange(len(episode_rewards)), episode_rewards)\n",
        "plots[1].set_ylabel(\"Loss per Step\")\n",
        "plots[1].set_xlabel(\"Step\")\n",
        "plots[1].plot(np.arange(len(losses)), losses)\n",
        "plots[2].set_ylabel(\"Averaged Loss per Episode\")\n",
        "plots[2].set_xlabel(\"Episode\")\n",
        "plots[2].plot(np.arange(len(averaged_episode_losses)), averaged_episode_losses)\n",
        "plt.savefig(\"Storage/graphs.png\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "train_atari.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
