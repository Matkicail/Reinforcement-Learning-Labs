{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MJDesktop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\setuptools\\distutils_patch.py:25: UserWarning: Distutils was imported before Setuptools. This usage is discouraged and may exhibit undesirable behaviors or errors. Please use Setuptools' objects directly or at least import Setuptools first.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import OrderedDict, deque, namedtuple\n",
    "from typing import List, Tuple\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from pytorch_lightning.utilities import DistributedType\n",
    "from torch import Tensor, nn\n",
    "from torch.optim import Adam, Optimizer\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import IterableDataset\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "PATH_DATASETS = os.environ.get(\"PATH_DATASETS\", \".\")\n",
    "AVAIL_GPUS = min(1, torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parallel(nn.Module):\n",
    "\tdef __init__(self, *modules: torch.nn.Module):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.modules = modules\n",
    "\n",
    "\tdef forward(self, inputs):\n",
    "\t\treturn [module(inputs) for module in self.modules]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACNetwork(nn.Module):\n",
    "\t\"\"\"Simple MLP network.\"\"\"\n",
    "\n",
    "\tdef __init__(self, obs_size: int, n_actions: int, hidden_size: int = 128):\n",
    "\t\t\"\"\"\n",
    "\t\tArgs:\n",
    "\t\t\tobs_size: observation/state size of the environment\n",
    "\t\t\tn_actions: number of discrete actions available in the environment\n",
    "\t\t\thidden_size: size of hidden layers\n",
    "\t\t\"\"\"\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.net = nn.Sequential(\n",
    "\t\t\tnn.Linear(obs_size, hidden_size),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(hidden_size, hidden_size),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tParallel(\n",
    "\t\t\t\tnn.Linear(hidden_size, 1),\n",
    "\t\t\t\tnn.Sequential(\n",
    "\t\t\t\t\tnn.Linear(hidden_size, n_actions),\n",
    "\t\t\t\t\tnn.Softmax(),\n",
    "\t\t\t\t),\n",
    "\t\t\t),\n",
    "\t\t)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\treturn self.net(x.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experience = namedtuple(\n",
    "\t\"Experience\",\n",
    "\tfield_names=[\"state_value\", \"log_probability\", \"reward\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "\t\"\"\"Replay Buffer for storing past experiences allowing the agent to learn from them.\n",
    "\n",
    "\tArgs:\n",
    "\t\tcapacity: size of the buffer\n",
    "\t\"\"\"\n",
    "\n",
    "\tdef __init__(self, capacity: int) -> None:\n",
    "\t\tself.buffer = deque(maxlen=capacity)\n",
    "\n",
    "\tdef __len__(self) -> None:\n",
    "\t\treturn len(self.buffer)\n",
    "\n",
    "\tdef append(self, experience: Experience) -> None:\n",
    "\t\t\"\"\"Add experience to the buffer.\n",
    "\n",
    "\t\tArgs:\n",
    "\t\t\texperience: tuple (state_value, log_probability, reward)\n",
    "\t\t\"\"\"\n",
    "\t\tself.buffer.append(experience)\n",
    "\n",
    "\tdef clear(self) -> None:\n",
    "\t\tself.buffer.clear()\n",
    "\n",
    "\tdef sample(self, batch_size: int) -> Tuple:\n",
    "\t\tindices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "\t\tstate_values, log_probabilities, rewards = zip(*(self.buffer[idx] for idx in indices))\n",
    "\n",
    "\t\treturn (\n",
    "\t\t\tnp.array(state_values, dtype=np.float32),\n",
    "\t\t\tnp.array(log_probabilities, dtype=np.float32),\n",
    "\t\t\tnp.array(rewards, dtype=np.float32),\n",
    "\t\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLDataset(IterableDataset):\n",
    "\t\"\"\"Iterable Dataset containing the ExperienceBuffer which will be updated with new experiences during training.\n",
    "\n",
    "\tArgs:\n",
    "\t\tbuffer: replay buffer\n",
    "\t\tsample_size: number of experiences to sample at a time\n",
    "\t\"\"\"\n",
    "\n",
    "\tdef __init__(self, buffer: ReplayBuffer, sample_size: int = 200) -> None:\n",
    "\t\tself.buffer = buffer\n",
    "\t\tself.sample_size = sample_size\n",
    "\n",
    "\tdef __iter__(self) -> Tuple:\n",
    "\t\tstate_values, log_probabilities, rewards = self.buffer.sample(self.sample_size)\n",
    "\t\tfor i in range(len(rewards)):\n",
    "\t\t\tyield state_values[i], log_probabilities[i], rewards[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACAgent:\n",
    "\t\"\"\"Base Agent class handeling the interaction with the environment.\"\"\"\n",
    "\n",
    "\tdef __init__(self, env: gym.Env, replay_buffer: ReplayBuffer) -> None:\n",
    "\t\t\"\"\"\n",
    "\t\tArgs:\n",
    "\t\t\tenv: training environment\n",
    "\t\t\"\"\"\n",
    "\t\tself.env = env\n",
    "\t\tself.replay_buffer = replay_buffer\n",
    "\t\tself.reset()\n",
    "\t\tself.state = self.env.reset()\n",
    "\n",
    "\tdef reset(self) -> None:\n",
    "\t\t\"\"\"Resents the environment and updates the state.\"\"\"\n",
    "\t\tself.state = self.env.reset()\n",
    "\n",
    "\tdef get_action(self, net: nn.Module, device: str) -> int:\n",
    "\t\t\"\"\"Using the given network, decide what action to carry out using an epsilon-greedy policy.\n",
    "\n",
    "\t\tArgs:\n",
    "\t\t\tnet: DQN network\n",
    "\t\t\tepsilon: value to determine likelihood of taking a random action\n",
    "\t\t\tdevice: current device\n",
    "\n",
    "\t\tReturns:\n",
    "\t\t\taction\n",
    "\t\t\"\"\"\n",
    "\t\t\n",
    "\t\tstate = torch.tensor([self.state])\n",
    "\n",
    "\t\tif device not in [\"cpu\"]:\n",
    "\t\t\tstate = state.cuda(device)\n",
    "\n",
    "\t\tstate_value, action_probabilities = net(state)\n",
    "\n",
    "\t\taction_distribution = Categorical(action_probabilities)\n",
    "\t\taction = action_distribution.sample()\n",
    "\t\tlog_probability = action_distribution.log_prob(action)\n",
    "\n",
    "\t\taction = int(action.item())\n",
    "\t\t# Unsure if it will work\n",
    "\t\tstate_value = float(state_value.item())\n",
    "\t\tlog_probability = float(log_probability.item())\n",
    "\n",
    "\t\treturn state_value, log_probability, action\n",
    "\n",
    "\t@torch.no_grad()\n",
    "\tdef play_step(\n",
    "\t\tself,\n",
    "\t\tnet: nn.Module,\n",
    "\t\tdevice: str = \"cpu\",\n",
    "\t) -> Tuple[float, bool]:\n",
    "\t\t\"\"\"Carries out a single interaction step between the agent and the environment.\n",
    "\n",
    "\t\tArgs:\n",
    "\t\t\tnet: DQN network\n",
    "\t\t\tepsilon: value to determine likelihood of taking a random action\n",
    "\t\t\tdevice: current device\n",
    "\n",
    "\t\tReturns:\n",
    "\t\t\treward, done\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\tstate_value, log_probability, action = self.get_action(net, device)\n",
    "\n",
    "\t\t# do step in the environment\n",
    "\t\tnew_state, reward, done, _ = self.env.step(action)\n",
    "\n",
    "\t\texp = Experience(state_value, log_probability, reward)\n",
    "\n",
    "\t\tself.replay_buffer.append(exp)\n",
    "\n",
    "\t\tself.state = new_state\n",
    "\t\tif done:\n",
    "\t\t\tself.reset()\n",
    "\t\treturn reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACNLightning(LightningModule):\n",
    "\t\"\"\"Basic DQN Model.\"\"\"\n",
    "\n",
    "\tdef __init__(\n",
    "\t\tself,\n",
    "\t\tbatch_size: int = 16,\n",
    "\t\tlr: float = 1e-2,\n",
    "\t\tenv: str = \"CartPole-v0\",\n",
    "\t\tgamma: float = 0.99,\n",
    "\t\tsync_rate: int = 10,\n",
    "\t\treplay_size: int = 1000,\n",
    "\t\twarm_start_size: int = 1000,\n",
    "\t\teps_last_frame: int = 1000,\n",
    "\t\teps_start: float = 1.0,\n",
    "\t\teps_end: float = 0.01,\n",
    "\t\tepisode_length: int = 200,\n",
    "\t\twarm_start_steps: int = 1000,\n",
    "\t) -> None:\n",
    "\t\t\"\"\"\n",
    "\t\tArgs:\n",
    "\t\t\tbatch_size: size of the batches\")\n",
    "\t\t\tlr: learning rate\n",
    "\t\t\tenv: gym environment tag\n",
    "\t\t\tgamma: discount factor\n",
    "\t\t\tsync_rate: how many frames do we update the target network\n",
    "\t\t\treplay_size: capacity of the replay buffer\n",
    "\t\t\twarm_start_size: how many samples do we use to fill our buffer at the start of training\n",
    "\t\t\teps_last_frame: what frame should epsilon stop decaying\n",
    "\t\t\teps_start: starting value of epsilon\n",
    "\t\t\teps_end: final value of epsilon\n",
    "\t\t\tepisode_length: max length of an episode\n",
    "\t\t\twarm_start_steps: max episode reward in the environment\n",
    "\t\t\"\"\"\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.save_hyperparameters()\n",
    "\n",
    "\t\tself.env = gym.make(self.hparams.env)\n",
    "\t\tobs_size = self.env.observation_space.shape[0]\n",
    "\t\tn_actions = self.env.action_space.n\n",
    "\n",
    "\t\tself.net = DQN(obs_size, n_actions)\n",
    "\t\tself.target_net = DQN(obs_size, n_actions)\n",
    "\n",
    "\t\tself.buffer = ReplayBuffer(self.hparams.replay_size)\n",
    "\t\tself.agent = Agent(self.env, self.buffer)\n",
    "\t\tself.total_reward = 0\n",
    "\t\tself.episode_reward = 0\n",
    "\t\tself.populate(self.hparams.warm_start_steps)\n",
    "\n",
    "\tdef populate(self, steps: int = 1000) -> None:\n",
    "\t\t\"\"\"Carries out several random steps through the environment to initially fill up the replay buffer with\n",
    "\t\texperiences.\n",
    "\n",
    "\t\tArgs:\n",
    "\t\t\tsteps: number of random steps to populate the buffer with\n",
    "\t\t\"\"\"\n",
    "\t\tfor i in range(steps):\n",
    "\t\t\tself.agent.play_step(self.net, epsilon=1.0)\n",
    "\n",
    "\tdef training_batch(self,) -> None:\n",
    "\t\t\"\"\"Carries out several random steps through the environment to initially fill up the replay buffer with\n",
    "\t\texperiences.\n",
    "\n",
    "\t\tArgs:\n",
    "\t\t\tsteps: number of random steps to populate the buffer with\n",
    "\t\t\"\"\"\n",
    "\t\tfor i in range(steps):\n",
    "\t\t\tself.agent.play_step(self.net, epsilon=1.0)\n",
    "\n",
    "\tdef forward(self, x: Tensor) -> Tensor:\n",
    "\t\t\"\"\"Passes in a state x through the network and gets the q_values of each action as an output.\n",
    "\n",
    "\t\tArgs:\n",
    "\t\t\tx: environment state\n",
    "\n",
    "\t\tReturns:\n",
    "\t\t\tq values\n",
    "\t\t\"\"\"\n",
    "\t\toutput = self.net(x)\n",
    "\t\treturn output\n",
    "\n",
    "\tdef dqn_mse_loss(self, batch: Tuple[Tensor, Tensor]) -> Tensor:\n",
    "\t\t\"\"\"Calculates the mse loss using a mini batch from the replay buffer.\n",
    "\n",
    "\t\tArgs:\n",
    "\t\t\tbatch: current mini batch of replay data\n",
    "\n",
    "\t\tReturns:\n",
    "\t\t\tloss\n",
    "\t\t\"\"\"\n",
    "\t\tstates, actions, rewards, dones, next_states = batch\n",
    "\n",
    "\t\tstate_action_values = self.net(states).gather(1, actions.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tnext_state_values = self.target_net(next_states).max(1)[0]\n",
    "\t\t\tnext_state_values[dones] = 0.0\n",
    "\t\t\tnext_state_values = next_state_values.detach()\n",
    "\n",
    "\t\texpected_state_action_values = next_state_values * self.hparams.gamma + rewards\n",
    "\n",
    "\t\treturn nn.MSELoss()(state_action_values, expected_state_action_values)\n",
    "\n",
    "\tdef training_step(self, batch: Tuple[Tensor, Tensor], nb_batch) -> OrderedDict:\n",
    "\t\t\"\"\"Carries out a single step through the environment to update the replay buffer. Then calculates loss\n",
    "\t\tbased on the minibatch recieved.\n",
    "\n",
    "\t\tArgs:\n",
    "\t\t\tbatch: current mini batch of replay data\n",
    "\t\t\tnb_batch: batch number\n",
    "\n",
    "\t\tReturns:\n",
    "\t\t\tTraining loss and log metrics\n",
    "\t\t\"\"\"\n",
    "\t\tdevice = self.get_device(batch)\n",
    "\t\tepsilon = max(\n",
    "\t\t\tself.hparams.eps_end,\n",
    "\t\t\tself.hparams.eps_start - self.global_step + 1 / self.hparams.eps_last_frame,\n",
    "\t\t)\n",
    "\n",
    "\t\t# step through environment with agent\n",
    "\t\treward, done = self.agent.play_step(self.net, epsilon, device)\n",
    "\t\tself.episode_reward += reward\n",
    "\n",
    "\t\t# calculates training loss\n",
    "\t\tloss = self.dqn_mse_loss(batch)\n",
    "\n",
    "\t\tif self.trainer._distrib_type in {DistributedType.DP, DistributedType.DDP2}:\n",
    "\t\t\tloss = loss.unsqueeze(0)\n",
    "\n",
    "\t\tif done:\n",
    "\t\t\tself.total_reward = self.episode_reward\n",
    "\t\t\tself.episode_reward = 0\n",
    "\n",
    "\t\t# Soft update of target network\n",
    "\t\tif self.global_step % self.hparams.sync_rate == 0:\n",
    "\t\t\tself.target_net.load_state_dict(self.net.state_dict())\n",
    "\n",
    "\t\tlog = {\n",
    "\t\t\t\"total_reward\": torch.tensor(self.total_reward).to(device),\n",
    "\t\t\t\"reward\": torch.tensor(reward).to(device),\n",
    "\t\t\t\"train_loss\": loss,\n",
    "\t\t}\n",
    "\t\tstatus = {\n",
    "\t\t\t\"steps\": torch.tensor(self.global_step).to(device),\n",
    "\t\t\t\"total_reward\": torch.tensor(self.total_reward).to(device),\n",
    "\t\t}\n",
    "\n",
    "\t\treturn OrderedDict({\"loss\": loss, \"log\": log, \"progress_bar\": status})\n",
    "\n",
    "\tdef configure_optimizers(self) -> List[Optimizer]:\n",
    "\t\t\"\"\"Initialize Adam optimizer.\"\"\"\n",
    "\t\toptimizer = Adam(self.net.parameters(), lr=self.hparams.lr)\n",
    "\t\treturn [optimizer]\n",
    "\n",
    "\tdef __dataloader(self) -> DataLoader:\n",
    "\t\t\"\"\"Initialize the Replay Buffer dataset used for retrieving experiences.\"\"\"\n",
    "\t\tdataset = RLDataset(self.buffer, self.hparams.episode_length)\n",
    "\t\tdataloader = DataLoader(\n",
    "\t\t\tdataset=dataset,\n",
    "\t\t\tbatch_size=self.hparams.batch_size,\n",
    "\t\t)\n",
    "\t\treturn dataloader\n",
    "\n",
    "\tdef train_dataloader(self) -> DataLoader:\n",
    "\t\t\"\"\"Get train loader.\"\"\"\n",
    "\t\treturn self.__dataloader()\n",
    "\n",
    "\tdef get_device(self, batch) -> str:\n",
    "\t\t\"\"\"Retrieve device currently being used by minibatch.\"\"\"\n",
    "\t\treturn batch[0].device.index if self.on_gpu else \"cpu\""
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "68a53a60f8dbe8ab693540a0dcf8d1cd7605000b65397d9117b399451174eee8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
