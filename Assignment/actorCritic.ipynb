{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from collections import OrderedDict, deque, namedtuple\n",
    "# from typing import List, Tuple\n",
    "\n",
    "# import gym\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# from pytorch_lightning import LightningModule, Trainer\n",
    "# from pytorch_lightning.utilities import DistributedType\n",
    "# from torch import Tensor, nn\n",
    "# from torch.optim import Adam, Optimizer\n",
    "# from torch.utils.data import DataLoader\n",
    "# from torch.utils.data.dataset import IterableDataset\n",
    "# from torch.distributions import Categorical\n",
    "\n",
    "# PATH_DATASETS = os.environ.get(\"PATH_DATASETS\", \".\")\n",
    "# AVAIL_GPUS = min(1, torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Parallel(nn.Module):\n",
    "# \tdef __init__(self, *modules: torch.nn.Module):\n",
    "# \t\tsuper().__init__()\n",
    "# \t\tself.modules = modules\n",
    "\n",
    "# \tdef forward(self, inputs):\n",
    "# \t\treturn [module(inputs) for module in self.modules]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ACNetwork(nn.Module):\n",
    "# \t\"\"\"Simple MLP network.\"\"\"\n",
    "\n",
    "# \tdef __init__(self, obs_size: int, n_actions: int, hidden_size: int = 128):\n",
    "# \t\t\"\"\"\n",
    "# \t\tArgs:\n",
    "# \t\t\tobs_size: observation/state size of the environment\n",
    "# \t\t\tn_actions: number of discrete actions available in the environment\n",
    "# \t\t\thidden_size: size of hidden layers\n",
    "# \t\t\"\"\"\n",
    "# \t\tsuper().__init__()\n",
    "# \t\tself.net = nn.Sequential(\n",
    "# \t\t\tnn.Linear(obs_size, hidden_size),\n",
    "# \t\t\tnn.ReLU(),\n",
    "# \t\t\tnn.Linear(hidden_size, hidden_size),\n",
    "# \t\t\tnn.ReLU(),\n",
    "# \t\t\tParallel(\n",
    "# \t\t\t\tnn.Linear(hidden_size, 1),\n",
    "# \t\t\t\tnn.Sequential(\n",
    "# \t\t\t\t\tnn.Linear(hidden_size, n_actions),\n",
    "# \t\t\t\t\tnn.Softmax(),\n",
    "# \t\t\t\t),\n",
    "# \t\t\t),\n",
    "# \t\t)\n",
    "\n",
    "# \tdef forward(self, x):\n",
    "# \t\treturn self.net(x.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experience = namedtuple(\n",
    "# \t\"Experience\",\n",
    "# \tfield_names=[\"state_value\", \"log_probability\", \"reward\"],\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ReplayBuffer:\n",
    "# \t\"\"\"Replay Buffer for storing past experiences allowing the agent to learn from them.\n",
    "\n",
    "# \tArgs:\n",
    "# \t\tcapacity: size of the buffer\n",
    "# \t\"\"\"\n",
    "\n",
    "# \tdef __init__(self, capacity: int) -> None:\n",
    "# \t\tself.buffer = deque(maxlen=capacity)\n",
    "\n",
    "# \tdef __len__(self) -> None:\n",
    "# \t\treturn len(self.buffer)\n",
    "\n",
    "# \tdef append(self, experience: Experience) -> None:\n",
    "# \t\t\"\"\"Add experience to the buffer.\n",
    "\n",
    "# \t\tArgs:\n",
    "# \t\t\texperience: tuple (state_value, log_probability, reward)\n",
    "# \t\t\"\"\"\n",
    "# \t\tself.buffer.append(experience)\n",
    "\n",
    "# \tdef clear(self) -> None:\n",
    "# \t\tself.buffer.clear()\n",
    "\n",
    "# \tdef sample(self, batch_size: int) -> Tuple:\n",
    "# \t\tindices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "# \t\tstate_values, log_probabilities, rewards = zip(*(self.buffer[idx] for idx in indices))\n",
    "\n",
    "# \t\treturn (\n",
    "# \t\t\tnp.array(state_values, dtype=np.float32),\n",
    "# \t\t\tnp.array(log_probabilities, dtype=np.float32),\n",
    "# \t\t\tnp.array(rewards, dtype=np.float32),\n",
    "# \t\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class RLDataset(IterableDataset):\n",
    "# \t\"\"\"Iterable Dataset containing the ExperienceBuffer which will be updated with new experiences during training.\n",
    "\n",
    "# \tArgs:\n",
    "# \t\tbuffer: replay buffer\n",
    "# \t\tsample_size: number of experiences to sample at a time\n",
    "# \t\"\"\"\n",
    "\n",
    "# \tdef __init__(self, buffer: ReplayBuffer, sample_size: int = 200) -> None:\n",
    "# \t\tself.buffer = buffer\n",
    "# \t\tself.sample_size = sample_size\n",
    "\n",
    "# \tdef __iter__(self) -> Tuple:\n",
    "# \t\tstate_values, log_probabilities, rewards = self.buffer.sample(self.sample_size)\n",
    "# \t\tfor i in range(len(rewards)):\n",
    "# \t\t\tyield state_values[i], log_probabilities[i], rewards[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ACAgent:\n",
    "# \t\"\"\"Base Agent class handeling the interaction with the environment.\"\"\"\n",
    "\n",
    "# \tdef __init__(self, env: gym.Env, replay_buffer: ReplayBuffer) -> None:\n",
    "# \t\t\"\"\"\n",
    "# \t\tArgs:\n",
    "# \t\t\tenv: training environment\n",
    "# \t\t\"\"\"\n",
    "# \t\tself.env = env\n",
    "# \t\tself.replay_buffer = replay_buffer\n",
    "# \t\tself.reset()\n",
    "# \t\tself.state = self.env.reset()\n",
    "\n",
    "# \tdef reset(self) -> None:\n",
    "# \t\t\"\"\"Resents the environment and updates the state.\"\"\"\n",
    "# \t\tself.state = self.env.reset()\n",
    "\n",
    "# \tdef get_action(self, net: nn.Module, device: str) -> int:\n",
    "# \t\t\"\"\"Using the given network, decide what action to carry out using an epsilon-greedy policy.\n",
    "\n",
    "# \t\tArgs:\n",
    "# \t\t\tnet: DQN network\n",
    "# \t\t\tepsilon: value to determine likelihood of taking a random action\n",
    "# \t\t\tdevice: current device\n",
    "\n",
    "# \t\tReturns:\n",
    "# \t\t\taction\n",
    "# \t\t\"\"\"\n",
    "\t\t\n",
    "# \t\tstate = torch.tensor([self.state])\n",
    "\n",
    "# \t\tif device not in [\"cpu\"]:\n",
    "# \t\t\tstate = state.cuda(device)\n",
    "\n",
    "# \t\tstate_value, action_probabilities = net(state)\n",
    "\n",
    "# \t\taction_distribution = Categorical(action_probabilities)\n",
    "# \t\taction = action_distribution.sample()\n",
    "# \t\tlog_probability = action_distribution.log_prob(action)\n",
    "\n",
    "# \t\taction = int(action.item())\n",
    "# \t\t# Unsure if it will work\n",
    "# \t\tstate_value = float(state_value.item())\n",
    "# \t\tlog_probability = float(log_probability.item())\n",
    "\n",
    "# \t\treturn state_value, log_probability, action\n",
    "\n",
    "# \t@torch.no_grad()\n",
    "# \tdef play_step(\n",
    "# \t\tself,\n",
    "# \t\tnet: nn.Module,\n",
    "# \t\tdevice: str = \"cpu\",\n",
    "# \t) -> Tuple[float, bool]:\n",
    "# \t\t\"\"\"Carries out a single interaction step between the agent and the environment.\n",
    "\n",
    "# \t\tArgs:\n",
    "# \t\t\tnet: DQN network\n",
    "# \t\t\tepsilon: value to determine likelihood of taking a random action\n",
    "# \t\t\tdevice: current device\n",
    "\n",
    "# \t\tReturns:\n",
    "# \t\t\treward, done\n",
    "# \t\t\"\"\"\n",
    "\n",
    "# \t\tstate_value, log_probability, action = self.get_action(net, device)\n",
    "\n",
    "# \t\t# do step in the environment\n",
    "# \t\tnew_state, reward, done, _ = self.env.step(action)\n",
    "\n",
    "# \t\texp = Experience(state_value, log_probability, reward)\n",
    "\n",
    "# \t\tself.replay_buffer.append(exp)\n",
    "\n",
    "# \t\tself.state = new_state\n",
    "# \t\tif done:\n",
    "# \t\t\tself.reset()\n",
    "# \t\treturn reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ACNLightning(LightningModule):\n",
    "# \t\"\"\"Basic DQN Model.\"\"\"\n",
    "\n",
    "# \tdef __init__(\n",
    "# \t\tself,\n",
    "# \t\tbatch_size: int = 16,\n",
    "# \t\tlr: float = 1e-2,\n",
    "# \t\tenv: str = \"CartPole-v0\",\n",
    "# \t\tgamma: float = 0.99,\n",
    "# \t\tsync_rate: int = 10,\n",
    "# \t\treplay_size: int = 1000,\n",
    "# \t\twarm_start_size: int = 1000,\n",
    "# \t\teps_last_frame: int = 1000,\n",
    "# \t\teps_start: float = 1.0,\n",
    "# \t\teps_end: float = 0.01,\n",
    "# \t\tepisode_length: int = 200,\n",
    "# \t\twarm_start_steps: int = 1000,\n",
    "# \t) -> None:\n",
    "# \t\t\"\"\"\n",
    "# \t\tArgs:\n",
    "# \t\t\tbatch_size: size of the batches\")\n",
    "# \t\t\tlr: learning rate\n",
    "# \t\t\tenv: gym environment tag\n",
    "# \t\t\tgamma: discount factor\n",
    "# \t\t\tsync_rate: how many frames do we update the target network\n",
    "# \t\t\treplay_size: capacity of the replay buffer\n",
    "# \t\t\twarm_start_size: how many samples do we use to fill our buffer at the start of training\n",
    "# \t\t\teps_last_frame: what frame should epsilon stop decaying\n",
    "# \t\t\teps_start: starting value of epsilon\n",
    "# \t\t\teps_end: final value of epsilon\n",
    "# \t\t\tepisode_length: max length of an episode\n",
    "# \t\t\twarm_start_steps: max episode reward in the environment\n",
    "# \t\t\"\"\"\n",
    "# \t\tsuper().__init__()\n",
    "# \t\tself.save_hyperparameters()\n",
    "\n",
    "# \t\tself.env = gym.make(self.hparams.env)\n",
    "# \t\tobs_size = self.env.observation_space.shape[0]\n",
    "# \t\tn_actions = self.env.action_space.n\n",
    "\n",
    "# \t\tself.net = DQN(obs_size, n_actions)\n",
    "# \t\tself.target_net = DQN(obs_size, n_actions)\n",
    "\n",
    "# \t\tself.buffer = ReplayBuffer(self.hparams.replay_size)\n",
    "# \t\tself.agent = Agent(self.env, self.buffer)\n",
    "# \t\tself.total_reward = 0\n",
    "# \t\tself.episode_reward = 0\n",
    "# \t\tself.populate(self.hparams.warm_start_steps)\n",
    "\n",
    "# \tdef populate(self, steps: int = 1000) -> None:\n",
    "# \t\t\"\"\"Carries out several random steps through the environment to initially fill up the replay buffer with\n",
    "# \t\texperiences.\n",
    "\n",
    "# \t\tArgs:\n",
    "# \t\t\tsteps: number of random steps to populate the buffer with\n",
    "# \t\t\"\"\"\n",
    "# \t\tfor i in range(steps):\n",
    "# \t\t\tself.agent.play_step(self.net, epsilon=1.0)\n",
    "\n",
    "# \tdef training_batch(self,) -> None:\n",
    "# \t\t\"\"\"Carries out several random steps through the environment to initially fill up the replay buffer with\n",
    "# \t\texperiences.\n",
    "\n",
    "# \t\tArgs:\n",
    "# \t\t\tsteps: number of random steps to populate the buffer with\n",
    "# \t\t\"\"\"\n",
    "# \t\tfor i in range(steps):\n",
    "# \t\t\tself.agent.play_step(self.net, epsilon=1.0)\n",
    "\n",
    "# \tdef forward(self, x: Tensor) -> Tensor:\n",
    "# \t\t\"\"\"Passes in a state x through the network and gets the q_values of each action as an output.\n",
    "\n",
    "# \t\tArgs:\n",
    "# \t\t\tx: environment state\n",
    "\n",
    "# \t\tReturns:\n",
    "# \t\t\tq values\n",
    "# \t\t\"\"\"\n",
    "# \t\toutput = self.net(x)\n",
    "# \t\treturn output\n",
    "\n",
    "# \tdef dqn_mse_loss(self, batch: Tuple[Tensor, Tensor]) -> Tensor:\n",
    "# \t\t\"\"\"Calculates the mse loss using a mini batch from the replay buffer.\n",
    "\n",
    "# \t\tArgs:\n",
    "# \t\t\tbatch: current mini batch of replay data\n",
    "\n",
    "# \t\tReturns:\n",
    "# \t\t\tloss\n",
    "# \t\t\"\"\"\n",
    "# \t\tstates, actions, rewards, dones, next_states = batch\n",
    "\n",
    "# \t\tstate_action_values = self.net(states).gather(1, actions.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "# \t\twith torch.no_grad():\n",
    "# \t\t\tnext_state_values = self.target_net(next_states).max(1)[0]\n",
    "# \t\t\tnext_state_values[dones] = 0.0\n",
    "# \t\t\tnext_state_values = next_state_values.detach()\n",
    "\n",
    "# \t\texpected_state_action_values = next_state_values * self.hparams.gamma + rewards\n",
    "\n",
    "# \t\treturn nn.MSELoss()(state_action_values, expected_state_action_values)\n",
    "\n",
    "# \tdef training_step(self, batch: Tuple[Tensor, Tensor], nb_batch) -> OrderedDict:\n",
    "# \t\t\"\"\"Carries out a single step through the environment to update the replay buffer. Then calculates loss\n",
    "# \t\tbased on the minibatch recieved.\n",
    "\n",
    "# \t\tArgs:\n",
    "# \t\t\tbatch: current mini batch of replay data\n",
    "# \t\t\tnb_batch: batch number\n",
    "\n",
    "# \t\tReturns:\n",
    "# \t\t\tTraining loss and log metrics\n",
    "# \t\t\"\"\"\n",
    "# \t\tdevice = self.get_device(batch)\n",
    "# \t\tepsilon = max(\n",
    "# \t\t\tself.hparams.eps_end,\n",
    "# \t\t\tself.hparams.eps_start - self.global_step + 1 / self.hparams.eps_last_frame,\n",
    "# \t\t)\n",
    "\n",
    "# \t\t# step through environment with agent\n",
    "# \t\treward, done = self.agent.play_step(self.net, epsilon, device)\n",
    "# \t\tself.episode_reward += reward\n",
    "\n",
    "# \t\t# calculates training loss\n",
    "# \t\tloss = self.dqn_mse_loss(batch)\n",
    "\n",
    "# \t\tif self.trainer._distrib_type in {DistributedType.DP, DistributedType.DDP2}:\n",
    "# \t\t\tloss = loss.unsqueeze(0)\n",
    "\n",
    "# \t\tif done:\n",
    "# \t\t\tself.total_reward = self.episode_reward\n",
    "# \t\t\tself.episode_reward = 0\n",
    "\n",
    "# \t\t# Soft update of target network\n",
    "# \t\tif self.global_step % self.hparams.sync_rate == 0:\n",
    "# \t\t\tself.target_net.load_state_dict(self.net.state_dict())\n",
    "\n",
    "# \t\tlog = {\n",
    "# \t\t\t\"total_reward\": torch.tensor(self.total_reward).to(device),\n",
    "# \t\t\t\"reward\": torch.tensor(reward).to(device),\n",
    "# \t\t\t\"train_loss\": loss,\n",
    "# \t\t}\n",
    "# \t\tstatus = {\n",
    "# \t\t\t\"steps\": torch.tensor(self.global_step).to(device),\n",
    "# \t\t\t\"total_reward\": torch.tensor(self.total_reward).to(device),\n",
    "# \t\t}\n",
    "\n",
    "# \t\treturn OrderedDict({\"loss\": loss, \"log\": log, \"progress_bar\": status})\n",
    "\n",
    "# \tdef configure_optimizers(self) -> List[Optimizer]:\n",
    "# \t\t\"\"\"Initialize Adam optimizer.\"\"\"\n",
    "# \t\toptimizer = Adam(self.net.parameters(), lr=self.hparams.lr)\n",
    "# \t\treturn [optimizer]\n",
    "\n",
    "# \tdef __dataloader(self) -> DataLoader:\n",
    "# \t\t\"\"\"Initialize the Replay Buffer dataset used for retrieving experiences.\"\"\"\n",
    "# \t\tdataset = RLDataset(self.buffer, self.hparams.episode_length)\n",
    "# \t\tdataloader = DataLoader(\n",
    "# \t\t\tdataset=dataset,\n",
    "# \t\t\tbatch_size=self.hparams.batch_size,\n",
    "# \t\t)\n",
    "# \t\treturn dataloader\n",
    "\n",
    "# \tdef train_dataloader(self) -> DataLoader:\n",
    "# \t\t\"\"\"Get train loader.\"\"\"\n",
    "# \t\treturn self.__dataloader()\n",
    "\n",
    "# \tdef get_device(self, batch) -> str:\n",
    "# \t\t\"\"\"Retrieve device currently being used by minibatch.\"\"\"\n",
    "# \t\treturn batch[0].device.index if self.on_gpu else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MJDesktop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\container.py:139: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "probabilities do not sum to 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\MJDESK~1\\AppData\\Local\\Temp/ipykernel_32312/2444284612.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 202\u001b[1;33m         \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\MJDESK~1\\AppData\\Local\\Temp/ipykernel_32312/2444284612.py\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSTEP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m                 \u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfinal_r\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcurrent_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mroll_out\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactor_network\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mSAMPLE_NUMS\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalue_network\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minit_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    155\u001b[0m                 \u001b[0minit_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcurrent_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m                 \u001b[0mactions_var\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\MJDESK~1\\AppData\\Local\\Temp/ipykernel_32312/2444284612.py\u001b[0m in \u001b[0;36mroll_out\u001b[1;34m(actor_network, task, sample_nums, value_network, init_state)\u001b[0m\n\u001b[0;32m    103\u001b[0m                 \u001b[0mlog_softmax_action\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mactor_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m                 \u001b[0msoftmax_action\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_softmax_action\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m                 \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msoftmax_action\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m                 \u001b[0mone_hot_action\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m                 \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: probabilities do not sum to 1"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "import gym\n",
    "\n",
    "# Hyper Parameters\n",
    "STATE_DIM = 4\n",
    "ACTION_DIM = 2\n",
    "STEP = 2000\n",
    "SAMPLE_NUMS = 30\n",
    "\n",
    "class ActorNetwork(nn.Module):\n",
    "\t\"\"\"Simple MLP network.\"\"\"\n",
    "\n",
    "\tdef __init__(self, obs_size: int, n_actions: int, hidden_size: int = 16):\n",
    "\t\t\"\"\"\n",
    "\t\tArgs:\n",
    "\t\t\tobs_size: observation/state size of the environment\n",
    "\t\t\tn_actions: number of discrete actions available in the environment\n",
    "\t\t\thidden_size: size of hidden layers\n",
    "\t\t\"\"\"\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.net = nn.Sequential(\n",
    "\t\t\tnn.Linear(obs_size, hidden_size),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(hidden_size, hidden_size),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(hidden_size, n_actions),\n",
    "\t\t\tnn.Softmax(),\n",
    "\t\t)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\treturn self.net(x.float())\n",
    "\n",
    "class CriticNetwork(nn.Module):\n",
    "\t\"\"\"Simple MLP network.\"\"\"\n",
    "\n",
    "\tdef __init__(self, obs_size: int, hidden_size: int = 16):\n",
    "\t\t\"\"\"\n",
    "\t\tArgs:\n",
    "\t\t\tobs_size: observation/state size of the environment\n",
    "\t\t\tn_actions: number of discrete actions available in the environment\n",
    "\t\t\thidden_size: size of hidden layers\n",
    "\t\t\"\"\"\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.net = nn.Sequential(\n",
    "\t\t\tnn.Linear(obs_size, hidden_size),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(hidden_size, hidden_size),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(hidden_size, 1),\n",
    "\t\t)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\treturn self.net(x.float())\n",
    "\n",
    "\n",
    "# class ActorNetwork(nn.Module):\n",
    "\n",
    "#     def __init__(self,input_size,hidden_size,action_size):\n",
    "#         super(ActorNetwork, self).__init__()\n",
    "#         self.fc1 = nn.Linear(input_size,hidden_size)\n",
    "#         self.fc2 = nn.Linear(hidden_size,hidden_size)\n",
    "#         self.fc3 = nn.Linear(hidden_size,action_size)\n",
    "\n",
    "#     def forward(self,x):\n",
    "#         out = F.relu(self.fc1(x))\n",
    "#         out = F.relu(self.fc2(out))\n",
    "#         out = F.log_softmax(self.fc3(out))\n",
    "#         return out\n",
    "\n",
    "# class ValueNetwork(nn.Module):\n",
    "\n",
    "#     def __init__(self,input_size,hidden_size,output_size):\n",
    "#         super(ValueNetwork, self).__init__()\n",
    "#         self.fc1 = nn.Linear(input_size,hidden_size)\n",
    "#         self.fc2 = nn.Linear(hidden_size,hidden_size)\n",
    "#         self.fc3 = nn.Linear(hidden_size,output_size)\n",
    "\n",
    "#     def forward(self,x):\n",
    "#         out = F.relu(self.fc1(x))\n",
    "#         out = F.relu(self.fc2(out))\n",
    "#         out = self.fc3(out)\n",
    "#         return out\n",
    "\n",
    "def roll_out(actor_network,task,sample_nums,value_network,init_state):\n",
    "\t#task.reset()\n",
    "\tstates = []\n",
    "\tactions = []\n",
    "\trewards = []\n",
    "\tis_done = False\n",
    "\tfinal_r = 0\n",
    "\tstate = init_state\n",
    "\n",
    "\tfor j in range(sample_nums):\n",
    "\t\tstates.append(state)\n",
    "\t\tlog_softmax_action = actor_network(Variable(torch.Tensor([state])))\n",
    "\t\tsoftmax_action = torch.exp(log_softmax_action)\n",
    "\t\taction = np.random.choice(task.action_space.n,p=softmax_action.cpu().data.numpy()[0])\n",
    "\t\tone_hot_action = [int(k == action) for k in range(task.action_space.n)]\n",
    "\t\tnext_state,reward,done,_ = task.step(action)\n",
    "\t\t#fix_reward = -10 if done else 1\n",
    "\t\tactions.append(one_hot_action)\n",
    "\t\trewards.append(reward)\n",
    "\t\tfinal_state = next_state\n",
    "\t\tstate = next_state\n",
    "\t\tif done:\n",
    "\t\t\tis_done = True\n",
    "\t\t\tstate = task.reset()\n",
    "\t\t\tbreak\n",
    "\tif not is_done:\n",
    "\t\tfinal_r = value_network(Variable(torch.Tensor([final_state]))).cpu().data.numpy()\n",
    "\n",
    "\treturn states,actions,rewards,final_r,state\n",
    "\n",
    "def discount_reward(r, gamma,final_r):\n",
    "\tdiscounted_r = np.zeros_like(r)\n",
    "\trunning_add = final_r\n",
    "\tfor t in reversed(range(0, len(r))):\n",
    "\t\trunning_add = running_add * gamma + r[t]\n",
    "\t\tdiscounted_r[t] = running_add\n",
    "\treturn discounted_r\n",
    "\n",
    "def main():\n",
    "\t# init a task generator for data fetching\n",
    "\ttask = gym.make(\"CartPole-v0\")\n",
    "\tinit_state = task.reset()\n",
    "\n",
    "\t# init value network\n",
    "\t# value_network = CriticNetwork(input_size = STATE_DIM,hidden_size = 40,output_size = 1)\n",
    "\t# value_network_optim = torch.optim.Adam(value_network.parameters(),lr=0.01)\n",
    "\n",
    "\tvalue_network = CriticNetwork(task.observation_space.shape[0])\n",
    "\tvalue_network_optim = torch.optim.Adam(value_network.parameters(),lr=0.01)\n",
    "\n",
    "\t# init actor network\n",
    "\t# actor_network = ActorNetwork(STATE_DIM,40,ACTION_DIM)\n",
    "\t# actor_network_optim = torch.optim.Adam(actor_network.parameters(),lr = 0.01)\n",
    "\n",
    "\tactor_network = ActorNetwork(task.observation_space.shape[0], task.action_space.n)\n",
    "\tactor_network_optim = torch.optim.Adam(actor_network.parameters(),lr = 0.01)\n",
    "\n",
    "\tsteps =[]\n",
    "\ttask_episodes =[]\n",
    "\ttest_results =[]\n",
    "\n",
    "\tfor step in range(STEP):\n",
    "\t\tstates,actions,rewards,final_r,current_state = roll_out(actor_network,task,SAMPLE_NUMS,value_network,init_state)\n",
    "\t\tinit_state = current_state\n",
    "\t\tactions_var = Variable(torch.Tensor(actions).view(-1,task.action_space.n))\n",
    "\t\tstates_var = Variable(torch.Tensor(states).view(-1,task.observation_space.shape[0]))\n",
    "\n",
    "\t\t# train actor network\n",
    "\t\tactor_network_optim.zero_grad()\n",
    "\t\tlog_softmax_actions = actor_network(states_var)\n",
    "\t\tvs = value_network(states_var).detach()\n",
    "\t\t# calculate qs\n",
    "\t\tqs = Variable(torch.Tensor(discount_reward(rewards,0.99,final_r)))\n",
    "\n",
    "\t\tadvantages = qs - vs\n",
    "\t\tactor_network_loss = - torch.mean(torch.sum(log_softmax_actions*actions_var,1)* advantages)\n",
    "\t\tactor_network_loss.backward()\n",
    "\t\ttorch.nn.utils.clip_grad_norm(actor_network.parameters(),0.5)\n",
    "\t\tactor_network_optim.step()\n",
    "\n",
    "\t\t# train value network\n",
    "\t\tvalue_network_optim.zero_grad()\n",
    "\t\ttarget_values = qs\n",
    "\t\tvalues = value_network(states_var)\n",
    "\t\tcriterion = nn.MSELoss()\n",
    "\t\tvalue_network_loss = criterion(values,target_values)\n",
    "\t\tvalue_network_loss.backward()\n",
    "\t\ttorch.nn.utils.clip_grad_norm(value_network.parameters(),0.5)\n",
    "\t\tvalue_network_optim.step()\n",
    "\n",
    "\t\t# Testing\n",
    "\t\tif (step + 1) % 50== 0:\n",
    "\t\t\t\tresult = 0\n",
    "\t\t\t\ttest_task = gym.make(\"CartPole-v0\")\n",
    "\t\t\t\tfor test_epi in range(10):\n",
    "\t\t\t\t\tstate = test_task.reset()\n",
    "\t\t\t\t\tfor test_step in range(200):\n",
    "\t\t\t\t\t\tsoftmax_action = torch.exp(actor_network(Variable(torch.Tensor([state]))))\n",
    "\t\t\t\t\t\t#print(softmax_action.data)\n",
    "\t\t\t\t\t\taction = np.argmax(softmax_action.data.numpy()[0])\n",
    "\t\t\t\t\t\tnext_state,reward,done,_ = test_task.step(action)\n",
    "\t\t\t\t\t\tresult += reward\n",
    "\t\t\t\t\t\tstate = next_state\n",
    "\t\t\t\t\t\tif done:\n",
    "\t\t\t\t\t\t\tbreak\n",
    "\t\t\t\tprint(\"step:\",step+1,\"test result:\",result/10.0)\n",
    "\t\t\t\tsteps.append(step+1)\n",
    "\t\t\t\ttest_results.append(result/10)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tmain()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "68a53a60f8dbe8ab693540a0dcf8d1cd7605000b65397d9117b399451174eee8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
