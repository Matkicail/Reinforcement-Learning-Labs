{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MJDesktop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\setuptools\\distutils_patch.py:25: UserWarning: Distutils was imported before Setuptools. This usage is discouraged and may exhibit undesirable behaviors or errors. Please use Setuptools' objects directly or at least import Setuptools first.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import OrderedDict, deque, namedtuple, defaultdict\n",
    "from typing import List, Tuple, Iterator\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from pytorch_lightning.callbacks import Callback\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from pytorch_lightning.utilities import DistributedType\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torch import Tensor, nn\n",
    "from torch.optim import Adam, Optimizer\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import IterableDataset\n",
    "from pl_bolts.models.rl import DQN\n",
    "\n",
    "PATH_DATASETS = os.environ.get(\"PATH_DATASETS\", \".\")\n",
    "AVAIL_GPUS = min(1, torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\t\"\"\"Simple MLP network.\"\"\"\n",
    "\n",
    "\tdef __init__(self, obs_size: int, n_actions: int, hidden_size: int = 128):\n",
    "\t\t\"\"\"\n",
    "\t\tArgs:\n",
    "\t\t\tobs_size: observation/state size of the environment\n",
    "\t\t\tn_actions: number of discrete actions available in the environment\n",
    "\t\t\thidden_size: size of hidden layers\n",
    "\t\t\"\"\"\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.net = nn.Sequential(\n",
    "\t\t\tnn.Linear(obs_size, hidden_size),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(hidden_size, n_actions),\n",
    "\t\t)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\treturn self.net(x.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Named tuple for storing experience steps gathered in training\n",
    "Experience = namedtuple(\n",
    "\t\"Experience\",\n",
    "\tfield_names=[\"state\", \"action\", \"reward\", \"done\", \"new_state\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "\t\"\"\"Replay Buffer for storing past experiences allowing the agent to learn from them.\n",
    "\n",
    "\tArgs:\n",
    "\t\tcapacity: size of the buffer\n",
    "\t\"\"\"\n",
    "\n",
    "\tdef __init__(self, capacity: int) -> None:\n",
    "\t\tself.buffer = deque(maxlen=capacity)\n",
    "\n",
    "\tdef __len__(self) -> None:\n",
    "\t\treturn len(self.buffer)\n",
    "\n",
    "\tdef append(self, experience: Experience) -> None:\n",
    "\t\t\"\"\"Add experience to the buffer.\n",
    "\n",
    "\t\tArgs:\n",
    "\t\t\texperience: tuple (state, action, reward, done, new_state)\n",
    "\t\t\"\"\"\n",
    "\t\tself.buffer.append(experience)\n",
    "\n",
    "\tdef sample(self, batch_size: int) -> Tuple:\n",
    "\t\tindices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "\t\tstates, actions, rewards, dones, next_states = zip(*(self.buffer[idx] for idx in indices))\n",
    "\n",
    "\t\treturn (\n",
    "\t\t\tnp.array(states),\n",
    "\t\t\t# np.array(actions),\n",
    "\t\t\tnp.array(actions, dtype=np.int64),\n",
    "\t\t\tnp.array(rewards, dtype=np.float32),\n",
    "\t\t\tnp.array(dones, dtype=np.bool),\n",
    "\t\t\tnp.array(next_states),\n",
    "\t\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLDataset(IterableDataset):\n",
    "\t\"\"\"Iterable Dataset containing the ExperienceBuffer which will be updated with new experiences during training.\n",
    "\n",
    "\tArgs:\n",
    "\t\tbuffer: replay buffer\n",
    "\t\tsample_size: number of experiences to sample at a time\n",
    "\t\"\"\"\n",
    "\n",
    "\tdef __init__(self, buffer: ReplayBuffer, sample_size: int = 200) -> None:\n",
    "\t\tself.buffer = buffer\n",
    "\t\tself.sample_size = sample_size\n",
    "\n",
    "\t# def __iter__(self) -> Tuple:\n",
    "\tdef __iter__(self) -> Iterator:\n",
    "\t# def __iter__(self):\n",
    "\t\tstates, actions, rewards, dones, new_states = self.buffer.sample(self.sample_size)\n",
    "\t\tfor i in range(len(dones)):\n",
    "\t\t\tyield states[i], actions[i], rewards[i], dones[i], new_states[i]\n",
    "\t\t\t# yield (states[i], actions[i], rewards[i], dones[i], new_states[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "\t\"\"\"Base Agent class handeling the interaction with the environment.\"\"\"\n",
    "\n",
    "\tdef __init__(self, env: gym.Env, replay_buffer: ReplayBuffer) -> None:\n",
    "\t\t\"\"\"\n",
    "\t\tArgs:\n",
    "\t\t\tenv: training environment\n",
    "\t\t\treplay_buffer: replay buffer storing experiences\n",
    "\t\t\"\"\"\n",
    "\t\tself.env = env\n",
    "\t\tself.replay_buffer = replay_buffer\n",
    "\t\tself.reset()\n",
    "\t\tself.state = self.env.reset()\n",
    "\n",
    "\tdef reset(self) -> None:\n",
    "\t\t\"\"\"Resents the environment and updates the state.\"\"\"\n",
    "\t\tself.state = self.env.reset()\n",
    "\n",
    "\tdef get_action(self, net: nn.Module, epsilon: float, device: str) -> int:\n",
    "\t\t\"\"\"Using the given network, decide what action to carry out using an epsilon-greedy policy.\n",
    "\n",
    "\t\tArgs:\n",
    "\t\t\tnet: DQN network\n",
    "\t\t\tepsilon: value to determine likelihood of taking a random action\n",
    "\t\t\tdevice: current device\n",
    "\n",
    "\t\tReturns:\n",
    "\t\t\taction\n",
    "\t\t\"\"\"\n",
    "\t\tif np.random.random() < epsilon:\n",
    "\t\t\taction = self.env.action_space.sample()\n",
    "\t\telse:\n",
    "\t\t\tstate = torch.tensor([self.state])\n",
    "\n",
    "\t\t\tif device not in [\"cpu\"]:\n",
    "\t\t\t\tstate = state.cuda(device)\n",
    "\n",
    "\t\t\tq_values = net(state)\n",
    "\t\t\t_, action = torch.max(q_values, dim=1)\n",
    "\t\t\taction = int(action.item())\n",
    "\n",
    "\t\treturn action\n",
    "\n",
    "\t@torch.no_grad()\n",
    "\tdef play_step(\n",
    "\t\tself,\n",
    "\t\tnet: nn.Module,\n",
    "\t\tepsilon: float = 0.0,\n",
    "\t\tdevice: str = \"cpu\",\n",
    "\t) -> Tuple[float, bool]:\n",
    "\t\t\"\"\"Carries out a single interaction step between the agent and the environment.\n",
    "\n",
    "\t\tArgs:\n",
    "\t\t\tnet: DQN network\n",
    "\t\t\tepsilon: value to determine likelihood of taking a random action\n",
    "\t\t\tdevice: current device\n",
    "\n",
    "\t\tReturns:\n",
    "\t\t\treward, done\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\taction = self.get_action(net, epsilon, device)\n",
    "\n",
    "\t\t# do step in the environment\n",
    "\t\tnew_state, reward, done, _ = self.env.step(action)\n",
    "\n",
    "\t\texp = Experience(self.state, action, reward, done, new_state)\n",
    "\n",
    "\t\tself.replay_buffer.append(exp)\n",
    "\n",
    "\t\tself.state = new_state\n",
    "\t\tif done:\n",
    "\t\t\tself.reset()\n",
    "\t\treturn reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNCallback(Callback):\n",
    "\t\n",
    "\tdef __init__(self):\n",
    "\t\tself.average_training_total_reward = 0.0\n",
    "\t\tself.training_current_epoch = 0\n",
    "\t\tself.training_current_global_step = 0\n",
    "\n",
    "\tdef merge_list_of_dictionaries(self, dict_list):\n",
    "\t\tnew_dict = {}\n",
    "\t\tfor d in dict_list:\n",
    "\t\t\tfor d_key in d:\n",
    "\t\t\t\tif d_key not in new_dict:\n",
    "\t\t\t\t\tnew_dict[d_key] = []\n",
    "\t\t\t\tnew_dict[d_key].append(d[d_key])\n",
    "\t\treturn new_dict\n",
    "\n",
    "\tdef update_running_total_reward_average(self, total_reward):\n",
    "\t\t# self.average_training_total_reward = self.average_training_total_reward + (total_reward - self.average_training_total_reward) / (1 + self.training_current_epoch)\n",
    "\t\tself.average_training_total_reward = self.average_training_total_reward + (total_reward - self.average_training_total_reward) / (1 + self.training_current_epoch)\n",
    "\n",
    "\tdef on_train_epoch_end(self, trainer, pl_module):\n",
    "\n",
    "\t\toutputs = pl_module.on_train_epoch_end_outputs\n",
    "\n",
    "\t\tlosses, logs = self.merge_list_of_dictionaries(outputs).values()\n",
    "\n",
    "\t\ttrain_losses, total_rewards, rewards, steps = self.merge_list_of_dictionaries(logs).values()\n",
    "\n",
    "\t\tlosses = torch.stack(losses)\n",
    "\n",
    "\t\ttrain_losses = torch.stack(train_losses)\n",
    "\t\ttotal_rewards = torch.stack(total_rewards)\n",
    "\t\trewards = torch.stack(rewards)\n",
    "\t\tsteps = torch.stack(steps)\n",
    "\n",
    "\t\taverage_loss = train_losses.mean()\n",
    "\t\ttotal_reward = total_rewards.max()\n",
    "\n",
    "\t\tself.update_running_total_reward_average(total_reward.item())\n",
    "\n",
    "\t\tfor index in np.arange(steps.size(dim=0)):\n",
    "\t\t\tpl_module.logger.experiment.add_scalar(\"Loss/Train\", train_losses[index], self.training_current_global_step)\n",
    "\t\t\tpl_module.logger.experiment.add_scalar(\"Reward/Train\", rewards[index], self.training_current_global_step)\n",
    "\t\t\tself.training_current_global_step += 1\n",
    "\n",
    "\t\tpl_module.logger.experiment.add_scalar(\"Average_Loss/Train\", average_loss, self.training_current_epoch)\n",
    "\t\tpl_module.logger.experiment.add_scalar(\"Total_Reward/Train\", total_reward, self.training_current_epoch)\n",
    "\t\tpl_module.logger.experiment.add_scalar(\"Average_Total_Reward/Train\", self.average_training_total_reward, self.training_current_epoch)\n",
    "\n",
    "\t\tself.training_current_epoch += 1\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNLightning(LightningModule):\n",
    "\t\"\"\"Basic DQN Model.\"\"\"\n",
    "\n",
    "\tdef __init__(\n",
    "\t\tself,\n",
    "\t\tbatch_size: int = 16,\n",
    "\t\tlr: float = 1e-2,\n",
    "\t\tenv: str = \"CartPole-v0\",\n",
    "\t\tgamma: float = 0.99,\n",
    "\t\tsync_rate: int = 10,\n",
    "\t\treplay_size: int = 1000,\n",
    "\t\twarm_start_size: int = 1000,\n",
    "\t\teps_last_frame: int = 1000,\n",
    "\t\teps_start: float = 1.0,\n",
    "\t\teps_end: float = 0.01,\n",
    "\t\tepisode_length: int = 200,\n",
    "\t\twarm_start_steps: int = 1000,\n",
    "\t) -> None:\n",
    "\t\t\"\"\"\n",
    "\t\tArgs:\n",
    "\t\t\tbatch_size: size of the batches\")\n",
    "\t\t\tlr: learning rate\n",
    "\t\t\tenv: gym environment tag\n",
    "\t\t\tgamma: discount factor\n",
    "\t\t\tsync_rate: how many frames do we update the target network\n",
    "\t\t\treplay_size: capacity of the replay buffer\n",
    "\t\t\twarm_start_size: how many samples do we use to fill our buffer at the start of training\n",
    "\t\t\teps_last_frame: what frame should epsilon stop decaying\n",
    "\t\t\teps_start: starting value of epsilon\n",
    "\t\t\teps_end: final value of epsilon\n",
    "\t\t\tepisode_length: max length of an episode\n",
    "\t\t\twarm_start_steps: max episode reward in the environment\n",
    "\t\t\"\"\"\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.save_hyperparameters()\n",
    "\n",
    "\t\tself.env = gym.make(self.hparams.env)\n",
    "\t\tobs_size = self.env.observation_space.shape[0]\n",
    "\t\tn_actions = self.env.action_space.n\n",
    "\n",
    "\t\tself.net = DQN(obs_size, n_actions)\n",
    "\t\tself.target_net = DQN(obs_size, n_actions)\n",
    "\n",
    "\t\tself.buffer = ReplayBuffer(self.hparams.replay_size)\n",
    "\t\tself.agent = Agent(self.env, self.buffer)\n",
    "\t\tself.total_reward = 0\n",
    "\t\tself.episode_reward = 0\n",
    "\t\t# self.average_training_total_reward = 0.0\n",
    "\t\t# self.training_current_epoch = 0\n",
    "\t\t# self.training_current_global_step = 0\n",
    "\t\tself.populate(self.hparams.warm_start_steps)\n",
    "\n",
    "\tdef populate(self, steps: int = 1000) -> None:\n",
    "\t\t\"\"\"Carries out several random steps through the environment to initially fill up the replay buffer with\n",
    "\t\texperiences.\n",
    "\n",
    "\t\tArgs:\n",
    "\t\t\tsteps: number of random steps to populate the buffer with\n",
    "\t\t\"\"\"\n",
    "\t\tfor i in range(steps):\n",
    "\t\t\tself.agent.play_step(self.net, epsilon=1.0)\n",
    "\n",
    "\tdef forward(self, x: Tensor) -> Tensor:\n",
    "\t\t\"\"\"Passes in a state x through the network and gets the q_values of each action as an output.\n",
    "\n",
    "\t\tArgs:\n",
    "\t\t\tx: environment state\n",
    "\n",
    "\t\tReturns:\n",
    "\t\t\tq values\n",
    "\t\t\"\"\"\n",
    "\t\toutput = self.net(x)\n",
    "\t\treturn output\n",
    "\n",
    "\tdef dqn_mse_loss(self, batch: Tuple[Tensor, Tensor]) -> Tensor:\n",
    "\t\t\"\"\"Calculates the mse loss using a mini batch from the replay buffer.\n",
    "\n",
    "\t\tArgs:\n",
    "\t\t\tbatch: current mini batch of replay data\n",
    "\n",
    "\t\tReturns:\n",
    "\t\t\tloss\n",
    "\t\t\"\"\"\n",
    "\t\tstates, actions, rewards, dones, next_states = batch\n",
    "\n",
    "\t\tstate_action_values = self.net(states).gather(1, actions.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tnext_state_values = self.target_net(next_states).max(1)[0]\n",
    "\t\t\tnext_state_values[dones] = 0.0\n",
    "\t\t\tnext_state_values = next_state_values.detach()\n",
    "\n",
    "\t\texpected_state_action_values = next_state_values * self.hparams.gamma + rewards\n",
    "\n",
    "\t\treturn nn.MSELoss()(state_action_values, expected_state_action_values)\n",
    "\n",
    "\t# def merge_list_of_dictionaries(self, dict_list):\n",
    "\t# \tnew_dict = {}\n",
    "\t# \tfor d in dict_list:\n",
    "\t# \t\tfor d_key in d:\n",
    "\t# \t\t\tif d_key not in new_dict:\n",
    "\t# \t\t\t\tnew_dict[d_key] = []\n",
    "\t# \t\t\tnew_dict[d_key].append(d[d_key])\n",
    "\t# \treturn new_dict\n",
    "\n",
    "\t# def update_running_total_reward_average(self, total_reward):\n",
    "\t\t# self.average_training_total_reward = self.average_training_total_reward + (total_reward - self.average_training_total_reward) / (1 + self.training_current_epoch)\n",
    "\t\t# self.average_training_total_reward = self.average_training_total_reward + (total_reward - self.average_training_total_reward) / (1 + self.current_epoch)\n",
    "\n",
    "\tdef training_step(self, batch: Tuple[Tensor, Tensor], nb_batch) -> OrderedDict:\n",
    "\t\t\"\"\"Carries out a single step through the environment to update the replay buffer. Then calculates loss\n",
    "\t\tbased on the minibatch recieved.\n",
    "\n",
    "\t\tArgs:\n",
    "\t\t\tbatch: current mini batch of replay data\n",
    "\t\t\tnb_batch: batch number\n",
    "\n",
    "\t\tReturns:\n",
    "\t\t\tTraining loss and log metrics\n",
    "\t\t\"\"\"\n",
    "\t\tdevice = self.get_device(batch)\n",
    "\t\tepsilon = max(\n",
    "\t\t\tself.hparams.eps_end,\n",
    "\t\t\tself.hparams.eps_start - (self.global_step + 1) / self.hparams.eps_last_frame,\n",
    "\t\t)\n",
    "\n",
    "\t\t# step through environment with agent\n",
    "\t\treward, done = self.agent.play_step(self.net, epsilon, device)\n",
    "\t\tself.episode_reward += reward\n",
    "\t\t# self.training_current_global_step += 1\n",
    "\n",
    "\t\t# calculates training loss\n",
    "\t\tloss = self.dqn_mse_loss(batch)\n",
    "\n",
    "\t\t# if self.trainer._distrib_type in {DistributedType.DP, DistributedType.DDP2}:\n",
    "\t\t# \tloss = loss.unsqueeze(0)\n",
    "\n",
    "\t\tif done:\n",
    "\t\t\tself.total_reward = self.episode_reward\n",
    "\t\t\tself.episode_reward = 0\n",
    "\n",
    "\t\t# Soft update of target network\n",
    "\t\tif self.global_step % self.hparams.sync_rate == 0:\n",
    "\t\t\tself.target_net.load_state_dict(self.net.state_dict())\n",
    "\n",
    "\t\t# log = {\n",
    "\t\t# \t\"total_reward\": torch.tensor(self.total_reward).to(device),\n",
    "\t\t# \t\"reward\": torch.tensor(reward).to(device),\n",
    "\t\t# \t\"train_loss\": loss,\n",
    "\t\t# }\n",
    "\t\t# status = {\n",
    "\t\t# \t\"steps\": torch.tensor(self.global_step).to(device),\n",
    "\t\t# \t\"total_reward\": torch.tensor(self.total_reward).to(device),\n",
    "\t\t# }\n",
    "\n",
    "\t\tlogs = {\n",
    "\t\t\t\"train_loss\": loss.detach(),\n",
    "\t\t\t\"total_reward\": torch.tensor(self.total_reward).to(device),\n",
    "\t\t\t\"reward\": torch.tensor(reward).to(device),\n",
    "\t\t\t\"step\": torch.tensor(self.global_step).to(device)\n",
    "\t\t\t# \"step\": torch.tensor(self.training_current_global_step).to(device)\n",
    "\t\t}\n",
    "\n",
    "\t\t# self.logger.experiment.add_scalar(\"Loss/Train\", loss, self.training_current_global_step)\n",
    "\t\t# self.logger.experiment.add_scalar(\"Reward/Train\", reward, self.training_current_global_step)\n",
    "\n",
    "\t\t# self.logger.experiment.add_scalar(\"Loss/Train\", loss, self.global_step)\n",
    "\t\t# self.logger.experiment.add_scalar(\"Reward/Train\", reward, self.global_step)\n",
    "\n",
    "\t\t# return OrderedDict({\"loss\": loss, \"log\": log, \"progress_bar\": status})\n",
    "\t\t# return OrderedDict({\"loss\": loss, \"log\": logs, \"progress_bar\": logs})\n",
    "\t\treturn OrderedDict({\"loss\": loss, \"log\": logs,})\n",
    "\t\t# return OrderedDict({\"loss\": loss})\n",
    "\n",
    "\t# def training_epoch_end(self, outputs) -> None:\n",
    "\t# \tlosses, logs = self.merge_list_of_dictionaries(outputs).values()\n",
    "\n",
    "\t# \ttrain_losses, total_rewards, rewards, steps = self.merge_list_of_dictionaries(logs).values()\n",
    "\n",
    "\t# \tlosses = torch.stack(losses)\n",
    "\n",
    "\t# \ttrain_losses = torch.stack(train_losses)\n",
    "\t# \ttotal_rewards = torch.stack(total_rewards)\n",
    "\t# \trewards = torch.stack(rewards)\n",
    "\t# \tsteps = torch.stack(steps)\n",
    "\n",
    "\t# \taverage_loss = train_losses.mean()\n",
    "\n",
    "\t# \ttotal_reward = total_rewards.max()\n",
    "\n",
    "\t# \tself.update_running_total_reward_average(total_reward.item())\n",
    "\n",
    "\t# \tself.logger.experiment.add_scalar(\"Average_Loss/Train\", average_loss, self.current_epoch)\n",
    "\t# \tself.logger.experiment.add_scalar(\"Total_Reward/Train\", total_reward, self.current_epoch)\n",
    "\t# \tself.logger.experiment.add_scalar(\"Average_Total_Reward/Train\", self.average_training_total_reward, self.current_epoch)\n",
    "\n",
    "\t# \tself.training_current_epoch += 1\n",
    "\n",
    "\tdef training_epoch_end(self, outputs) -> None:\n",
    "\t\tself.on_train_epoch_end_outputs = outputs\n",
    "\n",
    "\tdef configure_optimizers(self) -> List[Optimizer]:\n",
    "\t\t\"\"\"Initialize Adam optimizer.\"\"\"\n",
    "\t\toptimizer = Adam(self.net.parameters(), lr=self.hparams.lr)\n",
    "\t\treturn [optimizer]\n",
    "\n",
    "\tdef __dataloader(self) -> DataLoader:\n",
    "\t\t\"\"\"Initialize the Replay Buffer dataset used for retrieving experiences.\"\"\"\n",
    "\t\tdataset = RLDataset(self.buffer, self.hparams.episode_length)\n",
    "\t\tdataloader = DataLoader(\n",
    "\t\t\tdataset=dataset,\n",
    "\t\t\tbatch_size=self.hparams.batch_size,\n",
    "\t\t\tsampler=None,\n",
    "\t\t)\n",
    "\t\treturn dataloader\n",
    "\n",
    "\tdef train_dataloader(self) -> DataLoader:\n",
    "\t\t\"\"\"Get train loader.\"\"\"\n",
    "\t\treturn self.__dataloader()\n",
    "\n",
    "\tdef get_device(self, batch) -> str:\n",
    "\t\t\"\"\"Retrieve device currently being used by minibatch.\"\"\"\n",
    "\t\treturn batch[0].device.index if self.on_gpu else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name       | Type | Params\n",
      "------------------------------------\n",
      "0 | net        | DQN  | 898   \n",
      "1 | target_net | DQN  | 898   \n",
      "------------------------------------\n",
      "1.8 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.8 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "C:\\Users\\MJDesktop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:105: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9999: : 13it [00:00, 337.34it/s, loss=112, v_num=14]\n"
     ]
    }
   ],
   "source": [
    "model = DQNLightning()\n",
    "logger = TensorBoardLogger(\"tb_logs\", name=\"DQN_Cartpole\")\n",
    "\n",
    "trainer = Trainer(\n",
    "\tgpus=AVAIL_GPUS,\n",
    "\tmax_epochs=10000,\n",
    "\t# val_check_interval=100,\n",
    "\tlogger=logger,\n",
    "\tcallbacks=[DQNCallback()],\n",
    ")\n",
    "\n",
    "trainer.fit(model)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "68a53a60f8dbe8ab693540a0dcf8d1cd7605000b65397d9117b399451174eee8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
